(window["webpackJsonp_dubhe_web"]=window["webpackJsonp_dubhe_web"]||[]).push([["reinforceLearning-utils"],{1142:function(e,i,n){"use strict";n.r(i),n.d(i,"atlasAlgorithmList",(function(){return r}));var t=n("60fe"),r=[{en:"DQN",cn:"Deep Q-Network",description:["DQN 是 Q-learning 算法的升级版，使⽤深度神经⽹络来近似状态⾏为价值函数 Q。使⽤经验回放(Experience Replay) 来训练强化学习模型。使⽤两个 Q ⽹络，即添加 Target Network 来使得训练震荡发散可能性降低，更加稳定。"],paperLink:"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf",img:n("1575"),id:t["e"].DQN},{en:"D4PG",cn:"Distributed distributional deterministic policy gradients",description:["这篇 paper 是 DeepMind 提出的⼀种 DDPG 的改进，从 title 就可以看出来，这个⽅法⼤概率是 DeepMind 在业务中对 DDPG 结合了若⼲ advanced techniques 得到的 DDPG variant。","总的来说，D4PG 在 DDPG 的基础上，增加了以下⼏点：\n1）distributional RL \n2）distributed sampling (APEX)\n3）N-step returns    \n4）Prioritized Experience Replay (PER) "],paperLink:"https://arxiv.org/pdf/1804.08617.pdf",img:n("fee4"),id:t["e"].D4PG}]},1575:function(e,i){e.exports="//127.0.0.1:7201/img/DQN.7517eaf5.gif"},fee4:function(e,i){e.exports="//127.0.0.1:7201/img/D4PG.af3ef4b4.gif"}}]);